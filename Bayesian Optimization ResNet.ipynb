{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a2a8068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from scipy.stats import norm\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "from IPython.display import Image\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e41d060",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000\n",
    "\n",
    "# Use SubsetRandomSampler to randomly select a subset of the data\n",
    "sampler = torch.utils.data.SubsetRandomSampler(range(sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b69e0df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "batch_size = 64\n",
    "\n",
    "# Load Datasets\n",
    "train_dataset = FashionMNIST(root=\"./data\",\n",
    "                             train=True, download=True, transform=ToTensor())\n",
    "test_dataset = FashionMNIST(root=\"./data\",\n",
    "                            train=False, download=True, transform=ToTensor())\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=batch_size, sampler=sampler,\n",
    "                              shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                             batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24c3e07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a ResNet9 whose learning rate is to be optimized\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downsample=False):\n",
    "        super(ResBlock, self).__init__()\n",
    "        stride = 1\n",
    "        if downsample:\n",
    "            stride = 2\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels,\n",
    "                               kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels,\n",
    "                               kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.downsample = None\n",
    "        if downsample or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels,\n",
    "                          kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet9(nn.Module):\n",
    "    def __init__(self, in_channels=1, outputs=10):\n",
    "        super(ResNet9, self).__init__()\n",
    "        self.layer0 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            ResBlock(64, 64, downsample=False)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            ResBlock(64, 128, downsample=True),\n",
    "            ResBlock(128, 128, downsample=False)\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            ResBlock(128, 256, downsample=True),\n",
    "            ResBlock(256, 256, downsample=False)\n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            ResBlock(256, 512, downsample=True),\n",
    "            ResBlock(512, 512, downsample=False)\n",
    "        )\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(512, outputs)\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = self.layer0(input)\n",
    "        input = self.layer1(input)\n",
    "        input = self.layer2(input)\n",
    "        input = self.layer3(input)\n",
    "        input = self.layer4(input)\n",
    "        input = self.gap(input)\n",
    "        input = input.view(input.size(0), -1)\n",
    "        input = self.fc(input)\n",
    "\n",
    "        return input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60462f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gp(learning_rates, lr_eval):\n",
    "    \n",
    "    \"\"\"\n",
    "    Plot the observations, predicted mean, 95%-confidence interval, and acquisition function\n",
    "    for Gaussian Process optimization.\n",
    "\n",
    "    Arguments:\n",
    "        learning_rates (array-like): Array of learning rates/ observations.\n",
    "        lr_eval (array-like): Array of corresponding accuracy values.\n",
    "\n",
    "    \"\"\"\n",
    "        \n",
    "    test_inputs = np.linspace(lr_bounds[0], lr_bounds[1], 10000).reshape(-1, 1)\n",
    "    mean, std = gp.predict(test_inputs, return_std=True)\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 3))\n",
    "\n",
    "    # Plot observations and predicted mean with confidence interval\n",
    "    observations = ax1.scatter(learning_rates, lr_eval, marker='o',\n",
    "                               color='navy', label='Observations')\n",
    "    pred_mean = ax1.plot(test_inputs, mean, color='navy',\n",
    "                         linestyle='solid', label='Predicted Mean')\n",
    "    confidence_interval = ax1.fill_between(test_inputs.flatten(),\n",
    "                                           (mean - 1.96 * std).flatten(),\n",
    "                                           (mean + 1.96 * std).flatten(),\n",
    "                                           color='lightskyblue', alpha=0.4,\n",
    "                                           label='95% Confidence Interval')\n",
    "\n",
    "    # Set up secondary y-axis for expected improvement\n",
    "    ax2 = ax1.twinx()\n",
    "    acq_func_eval = np.array([expected_improvement(x.reshape(1, -1), lr_eval)\n",
    "                              for x in test_inputs])\n",
    "    ax2.plot(test_inputs, acq_func_eval, color='red', linestyle='dashed',\n",
    "             label='Acquisition Function')\n",
    "\n",
    "    # Set appropriate y-axis ticks for expected improvement\n",
    "    ymin = np.min(acq_func_eval)\n",
    "    ymax = np.max(acq_func_eval)\n",
    "    ax2.set_ylim(ymin, ymax)\n",
    "\n",
    "    # Set labels and combined legend\n",
    "    ax1.set_xlabel('Learning Rates')\n",
    "    ax1.set_ylabel('Observations /\\n Predicted Mean of Accuracy')\n",
    "    ax2.set_ylabel('Expected Improvement')\n",
    "\n",
    "    # Combine legends for both axes and position it outside on the right side\n",
    "    lines = [observations, *pred_mean, confidence_interval, *ax2.get_lines()]\n",
    "    labels = [line.get_label() for line in lines]\n",
    "    ax1.legend(lines, labels, loc='upper left', bbox_to_anchor=(1.2, 1),\n",
    "               borderaxespad=0.)\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "042f9f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = ResNet9()\n",
    "\n",
    "kernel = Matern(nu=2.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel)\n",
    "\n",
    "function_optimizations = 10 # Bayes' Optimization is performed ten times.\n",
    "epochs = 1 # The NN is only trained for one epoch to reduce computational time.\n",
    "lr_bounds = [0.0001, 0.01]\n",
    "\n",
    "initial_params = 1\n",
    "n_eval_points = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abb92ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_gradients(dataloader, model, criterion, optimizer):\n",
    "    \n",
    "    \"\"\"\n",
    "    Perform a forward pass, compute loss, update gradients, and optimize the model parameters.\n",
    "\n",
    "    Arguments:\n",
    "        dataloader (torch.utils.data.DataLoader): Data loader providing the training data.\n",
    "        model (torch.nn.Module): The neural network model.\n",
    "        criterion (torch.nn.Module): The loss function.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer used for updating gradients.\n",
    "\n",
    "    \"\"\"\n",
    "    for images, labels in dataloader:\n",
    "        pred = model(images)\n",
    "        loss = criterion(pred, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c27d151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(dataloader, model):\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute the accuracy of a model on a given dataset.\n",
    "\n",
    "    Arguments:\n",
    "        dataloader (torch.utils.data.DataLoader): Data loader providing the dataset.\n",
    "        model (torch.nn.Module): The neural network model.\n",
    "\n",
    "    Returns:\n",
    "        float: The accuracy of the model on the dataset.\n",
    "    \"\"\"\n",
    "    correct = sum((model(images).argmax(1) == labels).sum().item()\n",
    "                  for images, labels in dataloader)\n",
    "    accuracy = correct / len(dataloader.dataset)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cc7652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimization_step(model, epochs, x=0.001):\n",
    "    \n",
    "    \"\"\"\n",
    "    Update the gradients of the given model and compute its accuracy.\n",
    "\n",
    "    Arguments:\n",
    "        model (nn.Module): The model to optimize.\n",
    "        epochs (int): The number of training epochs.\n",
    "        x (float, optional): The learning rate for the optimizer. Defaults to 0.001.\n",
    "\n",
    "    Returns:\n",
    "        float: The accuracy achieved after the optimization step.\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "    for epoch in range(epochs):\n",
    "        update_gradients(train_dataloader, model, criterion, optimizer)\n",
    "        accuracy = objective_function(test_dataloader, model)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b1a3523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_improvement(X, lr_eval):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate the expected improvement for the given input points.\n",
    "\n",
    "    Arguments:\n",
    "        X (numpy.ndarray): Input points for which to calculate the expected improvement.\n",
    "        lr_eval (numpy.ndarray): Accuracy achieved with the learning rates.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The calculated expected improvement.\n",
    "    \"\"\"\n",
    "    mean, std = gp.predict(X, return_std=True)\n",
    "    std = np.maximum(std, 1e-12)\n",
    "    z = (mean - np.max(lr_eval)) / std\n",
    "    ei = (mean - np.max(lr_eval)) * norm.cdf(z) + std * norm.pdf(z)\n",
    "    return ei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4e8045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_next_point(learning_rates, lr_eval, expected_improvement):\n",
    "    \n",
    "    \"\"\"\n",
    "    Find the next learning rate and its corresponding acquisition value for optimization.\n",
    "\n",
    "    Args:\n",
    "        learning_rates (numpy.ndarray): Array of previous learning rates.\n",
    "        lr_eval (numpy.ndarray): Array of evaluation results of learning rates.\n",
    "        expected_improvement (function): Function to calculate expected improvement.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[numpy.ndarray, float]: Next learning rate and its corresponding acquisition value.\n",
    "    \"\"\"\n",
    "    best_acq_value = -np.inf\n",
    "    eval_points = np.random.uniform(lr_bounds[0], lr_bounds[1],\n",
    "                                    size=(n_eval_points, 1))\n",
    "    acq = np.array([expected_improvement(x.reshape(1, -1), lr_eval)\n",
    "                    for x in eval_points])\n",
    "    max_index = np.argmax(acq)\n",
    "    acq_max = acq[max_index]\n",
    "    best_acq_value = np.max(acq)\n",
    "    if acq_max >= best_acq_value:\n",
    "        next_lr = eval_points[max_index]\n",
    "        best_acq_value = acq_max\n",
    "    else:\n",
    "        next_lr = next_lr = learning_rates[np.argmax(lr_eval)]\n",
    "    return next_lr, best_acq_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c885f437",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def bayes_optimization(learning_rates, lr_eval):\n",
    "    \n",
    "    \"\"\"\n",
    "    Perform Bayesian optimization to find optimal learning rates.\n",
    "\n",
    "    Args:\n",
    "        learning_rates (numpy.ndarray): Array of initial learning rates.\n",
    "        lr_eval (numpy.ndarray): Array of evaluation results of learning rates.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[numpy.ndarray, numpy.ndarray, float]: Final learning rates, evaluation results, and best acquisition value.\n",
    "    \"\"\"\n",
    "    for i in range(function_optimizations):\n",
    "        next_lr, best_acq_value = find_next_point(learning_rates, lr_eval,\n",
    "                                                  expected_improvement)\n",
    "        next_eval = optimization_step(model=model, epochs=1, x=next_lr[0])\n",
    "        learning_rates = np.vstack((learning_rates, next_lr))\n",
    "        lr_eval = np.append(lr_eval, next_eval)\n",
    "        gp.fit(learning_rates, lr_eval)\n",
    "        print(f\"Iteration {i+1}: Learning Rate: {next_lr[0]}\"\n",
    "              \"\\nExpected Improvement: {best_acq_value[0]}\")\n",
    "        plot_gp(learning_rates, lr_eval)\n",
    "        # Save the plot to a PDF file\n",
    "        plt.savefig(f\"bo_plot_{i}.pdf\")\n",
    "        plt.savefig(f\"bo_plot_{i}.png\")\n",
    "        plt.close()\n",
    "    return learning_rates, lr_eval, best_acq_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f809037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization value of the Learning Rate:  [0.00553325]\n"
     ]
    }
   ],
   "source": [
    "# Compute an initial learning rate to initialize the Gaussian process.\n",
    "learning_rates = np.random.uniform(lr_bounds[0], lr_bounds[1],\n",
    "                                   size=(initial_params, 1))\n",
    "lr_eval = np.array([optimization_step(model=model, epochs=epochs,\n",
    "                                      x=x[0]) for x in learning_rates])\n",
    "gp.fit(learning_rates, lr_eval)\n",
    "\n",
    "print('Initialization value of the Learning Rate: ', learning_rates[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a84aa429",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Learning Rate: 0.00010540505248025698\n",
      "Expected Improvement: {best_acq_value[0]}\n",
      "Iteration 2: Learning Rate: 0.00012043989218677497\n",
      "Expected Improvement: {best_acq_value[0]}\n",
      "Iteration 3: Learning Rate: 0.0005093290859339112\n",
      "Expected Improvement: {best_acq_value[0]}\n",
      "Iteration 4: Learning Rate: 0.0003028172931723512\n",
      "Expected Improvement: {best_acq_value[0]}\n",
      "Iteration 5: Learning Rate: 0.0009964557255133714\n",
      "Expected Improvement: {best_acq_value[0]}\n",
      "Iteration 6: Learning Rate: 0.006101303449331678\n",
      "Expected Improvement: {best_acq_value[0]}\n",
      "Iteration 7: Learning Rate: 0.006602423978205236\n",
      "Expected Improvement: {best_acq_value[0]}\n",
      "Iteration 8: Learning Rate: 0.0071556287939000245\n",
      "Expected Improvement: {best_acq_value[0]}\n",
      "Iteration 9: Learning Rate: 0.007716375861178392\n",
      "Expected Improvement: {best_acq_value[0]}\n",
      "Iteration 10: Learning Rate: 0.00825649978201319\n",
      "Expected Improvement: {best_acq_value[0]}\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "# Perform Bayes' Optimization for ten iterations.\n",
    "learning_rates, lr_eval, best_acq_value = bayes_optimization(learning_rates,\n",
    "                                                             lr_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26cb287e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9145d6b914ed4f6aa1ced2024d099a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x03\\xe8\\x00\\x00\\x01,\\x08\\x06\\x00\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate a widget showing the resulting plots from the Bayesian optimization.\n",
    "image_files = []\n",
    "for i in range(function_optimizations-1):\n",
    "    image_file = f'bo_plot_{i}.png'\n",
    "    image_files.append(image_file)\n",
    "\n",
    "image_widgets = [widgets.Image(value=open(image_file, 'rb').read())\n",
    "                 for image_file in image_files]\n",
    "\n",
    "slideshow = widgets.Box(children=image_widgets)\n",
    "\n",
    "display(slideshow)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
